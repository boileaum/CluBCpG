#!/usr/bin/env python3

from MixtureAnalysis.ParseBam import BamFileReadParser
import os
import sys
import logging
from multiprocessing import Pool
import numpy as np
import pandas as pd
import argparse
from collections import defaultdict
import time

DEBUG = False

class CalculateCompleteBins:

    def __init__(self, bam_file: str, bin_size: int, output_directory: str, number_of_processors=1, mbias_read1_5=None, mbias_read1_3=None,
                 mbias_read2_5= None, mbias_read2_3=None):
        """
        This class is initialized with a path to a bam file and a bin size
        :param bam_file: One of the BAM files for analysis to be performed
        :param bin_size: Size of the bins for the analysis, integer
        :number_of_processors: How many CPUs to use for parallel computation, default=1
        """
        self.input_bam_file = bam_file
        self.bin_size = int(bin_size)
        self.number_of_processors = int(number_of_processors)
        self.output_directory = output_directory
        self.bins_no_reads = 0
        self.bins_yes_reads = 0

        self.mbias_read1_5 = mbias_read1_5
        self.mbias_read1_3 = mbias_read1_3
        self.mbias_read2_5 = mbias_read2_5
        self.mbias_read2_3 = mbias_read2_3

    def calculate_bin_coverage(self, bin):
        """
        Take a single bin, return a matrix
        :param bin: Bin should be passed as "Chr19_4343343"
        :return: pd.DataFrame with rows containing NaNs dropped
        """
        # Get reads from bam file
        parser = BamFileReadParser(self.input_bam_file, 20, self.mbias_read1_5, self.mbias_read1_3,
                                   self.mbias_read2_5, self.mbias_read2_3, no_overlap)
        # Split bin into parts
        chromosome, bin_location = bin.split("_")
        bin_location = int(bin_location)
        try:
            reads = parser.parse_reads(chromosome, bin_location-self.bin_size, bin_location)
            matrix = parser.create_matrix(reads)

        except ValueError:
            # No reads are within this window, do nothing
            # logging.info("No reads found for bin {}".format(bin))
            self.bins_no_reads += 1
            return None
        except:
            logging.error("Unknown error: {}".format(bin))
            return None

        # drop rows of ALL NaN
        matrix = matrix.dropna(how="all")

        matrix_has_unknowns = np.isnan(np.array(matrix)).any()

        # if impute flag is true, attempt to impute missing values
        try:
            if impute and matrix.shape[1] >=2 and matrix.shape[1] <= 6 and matrix_has_unknowns: # todo un-hardcode these values
                matrix_cols = matrix.columns
                imputer = ImputeWithCpGNet(cpg_density=matrix.shape[1], bin=bin, path_to_models=impute)
                matrix = imputer.impute_matrix(np.array(matrix.fillna(-1)), positions=np.array(matrix.columns))
                matrix = pd.DataFrame(matrix, columns=matrix_cols)
        except ValueError:
            logging.error("Error imputing at bin: {}".format(bin))

        # convert to data_frame of 1s and 0s, drop rows with NaN
        matrix = matrix.dropna()

        return bin, matrix

    def get_chromosome_lengths(self):
        """
        Get dictionary containing lengths of the chromosomes. Uses bam file for reference
        :return: Dictionary of chromosome lengths, ex: {"chrX": 222222}
        """
        parser = BamFileReadParser(self.input_bam_file, 20)
        return dict(zip(parser.OpenBamFile.references, parser.OpenBamFile.lengths))

    @staticmethod
    def remove_scaffolds(chromosome_len_dict):
        """
        Return a dict containing only the standard chromosomes starting with "chr"
        :param chromosome_len_dict: A dict generated by get_chromosome_lenghts()
        :return: a dict containing only chromosomes starting with "chr"
        """
        new_dict = dict(chromosome_len_dict)
        for key in chromosome_len_dict.keys():
            if not key.startswith("chr"):
                new_dict.pop(key)

        return new_dict

    def generate_bins_list(self, chromosome_len_dict: dict):
        """
        Get a dict of lists of all bins according to desired bin size for all chromosomes in the passed dict
        :param chromosome_len_dict: A dict of chromosome length sizes from get_chromosome_lenghts, cleaned up by remove_scaffolds() if desired
        :return: dict with each key being a chromosome. ex: chr1
        """
        all_bins = defaultdict(list)
        for key, value in chromosome_len_dict.items():
            bins = list(np.arange(self.bin_size, value + self.bin_size, self.bin_size))
            bins = ["_".join([key, str(x)]) for x in bins]
            all_bins[key].extend(bins)

        return all_bins

    def analyze_bins(self, individual_chrom=None):
        """
        Main function in class. Run the Complete analysis on the data
        :param individual_chrom: Chromosome to analyze: ie "chr7"
        :return: filename of the generated report
        """

        # Track the progress of the multiprocessing and output
        def track_progress(job, update_interval=60):
            while job._number_left > 0:
                logging.info("Tasks remaining = {0}".format(
                    job._number_left * job._chunksize))
                time.sleep(update_interval)


        # Get and clean dict of chromosome lenghts, convert to list of bins
        print("Getting Chromosome lengths from bam files...")
        chromosome_lengths = self.get_chromosome_lengths()
        chromosome_lengths = self.remove_scaffolds(chromosome_lengths)

        # If one chromosome was specified use only that chromosome
        if individual_chrom:
            new = dict()
            new[individual_chrom] = chromosome_lengths[individual_chrom]
            chromosome_lengths = new

        print("Generating bins for the entire genome...")
        bins_to_analyze = self.generate_bins_list(chromosome_lengths)

        # Set up for multiprocessing
        print("Beginning analysis of bins using {} processors.".format(self.number_of_processors))
        print("This will take awhile.....")

        # Loop over bin dict and pool.map them individually
        final_results = []
        for key in bins_to_analyze.keys():
            print("Analyzing chromosome {}".format(key))
            pool = Pool(processes=self.number_of_processors)
            results = pool.map_async(self.calculate_bin_coverage, bins_to_analyze[key])

            track_progress(results)

            # once done, get results
            results = results.get()

            final_results.extend(results)
            print("Finished chromosome {}".format(key))

        logging.info("Analysis complete")

        print("Complete.")

        # Write to output file
        output_file = os.path.join(self.output_directory, "CompleteBins.{}.{}.csv".format(os.path.basename(self.input_bam_file), individual_chrom))

        with open(output_file, "w") as out:
            for result in final_results:
                if result:
                    # bin
                    out.write(result[0] + ",")
                    # num of reads
                    out.write(str(result[1].shape[0]) + ",")
                    # num of CpGs
                    out.write(str(result[1].shape[1]) + "\n")

        logging.info("Full read coverage analysis complete!")
        return output_file


if __name__ == "__main__":

    def str2bool(v):
        if v.lower() == 'true':
            return True
        elif v.lower() == 'false':
            return False
        else:
            raise argparse.ArgumentTypeError("Boolean value expected.")

    # Input params
    arg_parser = argparse.ArgumentParser()
    arg_parser.add_argument("-a", "--input_bam_A",
                            help="First Input bam file, coordinate sorted with index present")
    arg_parser.add_argument("-o", "--output_dir",
                            help="Output directory to save figures, defaults to bam file loaction")
    arg_parser.add_argument("-bin_size", help="Size of bins to extract and analyze, default=100", default=100)
    arg_parser.add_argument("-n", "--num_processors",
                            help="Number of processors to use for analysis, default=1",
                            default=1)
    arg_parser.add_argument("-chr", "--chromosome",
                            help="Chromosome to analyze, example: 'chr19', required", default=None)

    arg_parser.add_argument("--read1_5", help="integer, read1 5' m-bias ignore bp, default=0", default=0)
    arg_parser.add_argument("--read1_3", help="integer, read1 3' m-bias ignore bp, default=0", default=0)
    arg_parser.add_argument("--read2_5", help="integer, read2 5' m-bias ignore bp, default=0", default=0)
    arg_parser.add_argument("--read2_3", help="integer, read2 3' m-bias ignore bp, default=0", default=0)
    arg_parser.add_argument("--no_overlap", help="bool, remove any overlap between paired reads and stitch"
                                                 " reads together when possible, default=True",
                            type=str2bool, const=True, default='True', nargs='?')
    arg_parser.add_argument("--train", help="folder to save model, After analysis, "
                                            "a machine learning model will be trained on the data. This model can the be  "
                                            "used with --impute to increase data coverage", default=None)

    arg_parser.add_argument("--impute", help="path to folder containing trained models. Use this after using --train to "
                                             "re-run analysis and attempting to increase coverage by imputing unknown values",
                            default=None)


    # Extract arguments from command line and set as correct types
    args = arg_parser.parse_args()

    input_bam_file = args.input_bam_A
    num_of_processors = int(args.num_processors)
    bin_size = int(args.bin_size)
    no_overlap = args.no_overlap
    impute = args.impute
    train = args.train

    # Get the mbias inputs and adjust to work correctly, 0s should be converted to None
    mbias_read1_5 = int(args.read1_5)
    mbias_read1_3 = int(args.read1_3)
    mbias_read2_5 = int(args.read2_5)
    mbias_read2_3 = int(args.read2_3)


    if args.chromosome:
        chrom_of_interest = args.chromosome
    else:
        chrom_of_interest = None

    # For now, this will be made required
    assert chrom_of_interest, "Chromosome to analyze must be specified"

    # Set output directory, or use bam file location if not specified
    if args.output_dir:
        BASE_DIR = args.output_dir
    else:
        BASE_DIR = os.path.dirname(input_bam_file)

    # Create output dir if it doesnt exist
    if not os.path.exists(BASE_DIR):
        os.makedirs(BASE_DIR)

    # Setup logging
    log_file = os.path.join(BASE_DIR, "CompleteBins.{}.{}.log".format(os.path.basename(input_bam_file), chrom_of_interest))
    print("Log file: {}".format(log_file), flush=True)
    logging.basicConfig(filename=log_file, level=logging.DEBUG)


    # Log run input params
    logging.info("Input file: {}".format(input_bam_file))
    logging.info("Chromosome specified: {}".format(chrom_of_interest))
    logging.info("Bin size: {}".format(bin_size))
    logging.info("Number of processors: {}".format(num_of_processors))
    logging.info("Fix overlapping reads: {}".format(no_overlap))

    if train:
        logging.info("Train set to True, I will try to train a model with CpGNet so you can run again with imputation")

    if impute:
        logging.info("Imputation set to True, will impute missing values in reads when possible")
        from MixtureAnalysis.ConnectToCpGNet import ImputeWithCpGNet
        ### CHANGE FOR NOW
        print("Im not updated for this yet. Fix me first")
        sys.exit(7)
    else:
        logging.info("Imputation set to False. I will simply drop reads with missing values")

    logging.info("M bias inputs ignoring the following:\nread 1 5': {}bp\n"
                 "read1 3': {}bp\nread2 5: {}bp\nread2 3': {}bp".format(mbias_read1_5, mbias_read1_3, mbias_read2_5, mbias_read2_3))

    # Perform the analysis
    calc = CalculateCompleteBins(input_bam_file, bin_size, BASE_DIR, num_of_processors,
                                 mbias_read1_5, mbias_read1_3, mbias_read2_5, mbias_read2_3)
    output_file = calc.analyze_bins(chrom_of_interest)


    # Train the model if directed to
    if train:
        logging.info("--train flag detected. Attempting to train a model...")
        from MixtureAnalysis.ConnectToCpGNet import TrainWithCpGNet

        # Read in outputfile as a dataframe
        complete_bins = pd.read_csv(output_file, header=None)
        complete_bins.columns = ['bin', 'reads', 'cpgs']

        # todo subset cpg_densities, load all matricies into memeory (if they fit) and train
        for i in range(2,6):
            # subset_bins
            logging.info("Training model of {} cpgs...".format(i))
            sub = complete_bins[complete_bins['cpgs'] == i]

